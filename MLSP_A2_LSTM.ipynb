{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cEceIBZDq2nO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import glob\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten,Dense,Conv2D,MaxPool2D,AvgPool2D,Bidirectional,LSTM,GlobalAveragePooling1D,TimeDistributed\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=sorted(glob.glob(\"/content/drive/MyDrive/MLSP_A2/X/*.npy\"))\n",
        "Y=sorted(glob.glob(\"/content/drive/MyDrive/MLSP_A2/Y/*.npy\"))"
      ],
      "metadata": {
        "id": "VleOoesdrDdM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test=sorted(glob.glob(\"/content/drive/MyDrive/MLSP_A2/Validation/X/*.npy\"))\n",
        "Y_test=sorted(glob.glob(\"/content/drive/MyDrive/MLSP_A2/Validation/Y/*.npy\"))"
      ],
      "metadata": {
        "id": "eo5ELOIZkSJE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalise(arr):\n",
        "  mu=np.mean(arr,axis=0)\n",
        "  std_dav=np.std(arr,axis=0)\n",
        "  normalised_arr=(arr-mu)/std_dav\n",
        "  return normalised_arr"
      ],
      "metadata": {
        "id": "4ZLl7wybrRl5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential([\n",
        "    #Conv1D(128,64,activation='relu',input_shape=(1000, 64),padding='same'),\n",
        "    #Conv1D(64,32,activation='relu',padding='same'),\n",
        "    #Conv1D(32,16,activation='relu',padding='same'),\n",
        "    Bidirectional(LSTM(256,input_shape=(1000,64),return_sequences=True)),\n",
        "    Bidirectional(LSTM(256,return_sequences=True)),\n",
        "    #GlobalAveragePooling1D(),\n",
        "    #AvgPool1D()\n",
        "    #Dense(512,activation='relu'),\n",
        "    TimeDistributed(Dense(256,activation='relu')),\n",
        "    TimeDistributed(Dense(64,activation='relu')),\n",
        "    TimeDistributed(Dense(11,activation='sigmoid'))\n",
        "])"
      ],
      "metadata": {
        "id": "EyNB7nADr9HQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BCE_loss=tf.keras.losses.BinaryCrossentropy()\n",
        "optimizer=tf.keras.optimizers.Adam(.0001)"
      ],
      "metadata": {
        "id": "jR0bE1YosKM8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=BCE_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[tf.keras.metrics.Precision(),tf.keras.metrics.Recall()]\n",
        ")"
      ],
      "metadata": {
        "id": "O5QN-WB6sLQI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "a=np.squeeze(np.load(X[0])).T\n",
        "a=a.reshape(20,50,64)\n",
        "print(a.shape)\n",
        "a=normalise(a)\n",
        "print(a.shape)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Hqw0Mwwxtk6h",
        "outputId": "d1cff6a4-49ea-4ef0-dd39-ffb726b81c77"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\na=np.squeeze(np.load(X[0])).T\\na=a.reshape(20,50,64)\\nprint(a.shape)\\na=normalise(a)\\nprint(a.shape)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'model(a).shape'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u0EQIY-HzKmD",
        "outputId": "d3113147-cad5-427f-8c54-4ae03cd1af14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'model(a).shape'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def train_step_l(data_path, label_path, optimizer, batch_size, shuffle = True):\n",
        "    index_list = np.arange(len(data_path))\n",
        "    all_y=[]\n",
        "    all_y_pred=[]\n",
        "    if shuffle:\n",
        "        np.random.shuffle(index_list)\n",
        "    #print(index_list)\n",
        "    for sample_data in range (int(np.floor(len(data_path)))):\n",
        "        #print(step)\n",
        "        x= np.squeeze(np.load(data_path[sample_data])).T\n",
        "        #print(x.shape)\n",
        "        x= x.reshape(20,50, 64)\n",
        "        y=np.load(label_path[sample_data]).T\n",
        "        #print(y.shape)\n",
        "        y = y.reshape(20,50,11)\n",
        "        x=normalise(x)\n",
        "        #print(batch_x.shape)\n",
        "        #if step==3:\n",
        "        #  break\n",
        "        #batch_x=np.squeeze(np.transpose(batch_x, (0,1,3,2)))\n",
        "        x=tf.convert_to_tensor(x)\n",
        "        y=tf.convert_to_tensor(y)\n",
        "        with tf.GradientTape() as tape:\n",
        "         y_pred=model(x)\n",
        "         #  print(y_pred.shape)\n",
        "         loss=BCE_loss(y_pred,y)\n",
        "        grads=tape.gradient(loss,model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
        "        if (sample_data+1)%1000==0:\n",
        "          print(\"Step = {}, Loss = {}\".format(sample_data + 1, loss))\n",
        "        y_pred=y_pred.numpy()\n",
        "        y_pred=y_pred.reshape(1000,11)\n",
        "        y_pred=np.round(y_pred)\n",
        "        #print(y_pred.shape)\n",
        "        y=y.numpy()\n",
        "        y=y.reshape(1000,11)\n",
        "        #print(y.shape)\n",
        "        all_y.append(y)\n",
        "        all_y_pred.append(y_pred)\n",
        "        \n",
        "    all_y=np.concatenate(all_y, axis=0)\n",
        "    all_y_pred=np.concatenate(all_y_pred,axis=0)\n",
        "    #print (all_y.shape)\n",
        "    #print (all_y_pred.shape)\n",
        "    classwise_precision=[]\n",
        "    classwise_recall=[]\n",
        "    classwise_f1=[]\n",
        "    for i in range(11):\n",
        "     Yk=all_y[:,i]\n",
        "     Ypredk=all_y_pred[:,i]\n",
        "     scores=precision_recall_fscore_support(Yk, Ypredk, average='macro')\n",
        "     classwise_precision.append(scores[0])\n",
        "     classwise_recall.append(scores[1])\n",
        "     classwise_f1.append(scores[2])\n",
        "    avg_precision=sum(classwise_precision)/len(classwise_precision)\n",
        "    avg_recall=sum(classwise_recall)/len(classwise_recall)\n",
        "    avg_f1=sum(classwise_f1)/len(classwise_f1)      \n",
        "    #print(\"classwise precision:{},avg precision:{},classwise recall:{},avg recall:{},classwise_f1:{},avg f1:{}\".format(classwise_precision,avg_precision,classwise_recall,avg_recall,classwise_f1,avg_f1))        \n",
        "\n",
        "    return  loss,classwise_precision,classwise_recall,classwise_f1,avg_precision,avg_recall,avg_f1\n",
        "'''"
      ],
      "metadata": {
        "id": "mMg6fG-8sV3t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_step(data_path, label_path, optimizer, epoch, shuffle = True):\n",
        "    index_list = np.arange(len(data_path))\n",
        "    \n",
        "    if shuffle:\n",
        "        np.random.shuffle(index_list)\n",
        "    #print(index_list)\n",
        "    for sample_data in range (int(np.floor(len(data_path)))):\n",
        "        #print(step)\n",
        "        x= np.squeeze(np.load(data_path[sample_data])).T\n",
        "        #print(x.shape)\n",
        "        x= x.reshape(20,50, 64)\n",
        "        y=np.load(label_path[sample_data]).T\n",
        "        #print(y.shape)\n",
        "        y = y.reshape(20,50,11)\n",
        "        x=normalise(x)\n",
        "        #print(batch_x.shape)\n",
        "        #if step==3:\n",
        "        #  break\n",
        "        x=tf.convert_to_tensor(x)\n",
        "        y=tf.convert_to_tensor(y)\n",
        "        with tf.GradientTape() as tape:\n",
        "         output=model(x)\n",
        "         #  print(output.shape)\n",
        "         loss=BCE_loss(output,y)\n",
        "        grads=tape.gradient(loss,model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
        "        if (sample_data+1)%1000==0:\n",
        "          print(\"Step = {}, Loss = {}\".format(sample_data + 1, loss))\n",
        "          model.save(f'/content/drive/MyDrive/MLSP_A2/FrameWise_Bi_LSTM_Modified_{epoch}_{sample_data}')\n",
        "       \n",
        "\n",
        "\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "r8oMKJMhjgYX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_step(data_path,label_path):\n",
        "  all_y=[]\n",
        "  all_y_pred=[]\n",
        "  for sample in range(len(data_path)):\n",
        "    x= np.squeeze(np.load(data_path[sample])).T\n",
        "    x= x.reshape(20,50, 64)\n",
        "    x=normalise(x)\n",
        "    y_pred=model(x)\n",
        "    y_pred=y_pred.numpy()\n",
        "    y_pred=y_pred.reshape(1000,11)\n",
        "    y_pred=y_pred.T\n",
        "    y_pred=np.round(y_pred)\n",
        "    #print(y_pred.shape)\n",
        "    y=np.load(label_path[sample])\n",
        "    #print(y.shape)\n",
        "    all_y.append(y.T)\n",
        "    all_y_pred.append(y_pred.T)\n",
        "    \n",
        "  all_y=np.concatenate(all_y, axis=0)\n",
        "  all_y_pred=np.concatenate(all_y_pred,axis=0)\n",
        "  #print(all_y.shape,all_y_pred.shape)  \n",
        "  \n",
        "  classwise_precision=[]\n",
        "  classwise_recall=[]\n",
        "  classwise_f1=[]\n",
        "  for i in range(11):\n",
        "    Yk=all_y[:,i]\n",
        "    Ypredk=all_y_pred[:,i]\n",
        "    scores=precision_recall_fscore_support(Yk, Ypredk, average='micro')\n",
        "    classwise_precision.append(scores[0])\n",
        "    classwise_recall.append(scores[1])\n",
        "    classwise_f1.append(scores[2])\n",
        "  avg_precision=sum(classwise_precision)/len(classwise_precision)\n",
        "  avg_recall=sum(classwise_recall)/len(classwise_recall)\n",
        "  avg_f1=sum(classwise_f1)/len(classwise_f1)      \n",
        "  print(\"classwise precision:{},avg precision:{},classwise recall:{},avg recall:{},classwise_f1:{},avg f1:{}\".format(classwise_precision,avg_precision,classwise_recall,avg_recall,classwise_f1,avg_f1))\n",
        "\n",
        "  return avg_precision, avg_recall, avg_f1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j1MdqNUxiu_8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=20\n",
        "'''\n",
        "Loss=[]\n",
        "Avg_precision=[]\n",
        "Avg_recall=[]\n",
        "Avg_f1=[]\n",
        "Epoch=[]\n",
        "'''\n",
        "for epoch in range(epochs):\n",
        "  loss =train_step(X,Y,optimizer,epoch)\n",
        "  print('Epoch : {}, Loss : {}'.format(epoch + 1, loss))\n",
        "\n",
        "  '''\n",
        "  avg_precision, avg_recall, avg_f1 = eval_step(X_test, Y_test)\n",
        "  \n",
        "  Epoch.append(epoch+1)\n",
        "  Loss.append(loss)\n",
        "  Avg_precision.append(avg_precision)\n",
        "  Avg_recall.append(avg_recall)\n",
        "  Avg_f1.append(avg_f1)\n",
        "  metrics=pd.DataFrame({\n",
        "    'Epoch':Epoch,\n",
        "    'Loss':Loss,\n",
        "    'Average Precision':Avg_precision,\n",
        "    'Average Recall':Avg_recall,\n",
        "    'Average F1':Avg_f1\n",
        "    })\n",
        "  metrics.to_csv(f'/content/drive/MyDrive/MLSP_A2/logs/BiLSTM_Metrics.csv')\n",
        "  '''\n",
        "  "
      ],
      "metadata": {
        "id": "Pmut-DgG1WX7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}